--- perf_takehome.py	2026-01-20 21:02:41
+++ solution.py	2026-01-20 21:02:21
@@ -169,9 +169,17 @@
         one_const = self.scratch_const(1, "one", init_slots)
         vlen_const = self.scratch_const(VLEN, "vlen", init_slots)
         vlen8_const = self.scratch_const(VLEN * NVECS, "vlen8", init_slots)
+        vlen4_const = self.scratch_const(VLEN * 4, "vlen4", init_slots)
         n_nodes_const = self.scratch_const(n_nodes, "n_nodes", init_slots)
         batch_end = self.scratch_const(batch_size, "batch_end", init_slots)
         
+        # Init two_vec and zero_vec
+        two_vec = self.alloc_scratch("two_vec", VLEN)
+        zero_vec = self.alloc_scratch("zero_vec", VLEN)
+        two_const = self.scratch_const(2, "two", init_slots)
+        init_slots.append(("valu", ("vbroadcast", two_vec, two_const)))
+        init_slots.append(("valu", ("vbroadcast", zero_vec, zero_const)))
+        
         forest_values_p = self.scratch_const(7, "forest_values_p", init_slots)
         inp_indices_p = self.scratch_const(7 + n_nodes, "inp_indices_p", init_slots)
         inp_values_p = self.scratch_const(7 + n_nodes + batch_size, "inp_values_p", init_slots)
@@ -207,10 +215,7 @@
             init_slots.append(("valu", ("vbroadcast", v1, c1)))
             init_slots.append(("valu", ("vbroadcast", v3, c3)))
             hash_const_vecs.append((op1, op2, op3, v1, v3))
-
-        # Pointer offset constants for independent pointer setup
-        ptr_offsets = [self.scratch_const(i * VLEN, slots=init_slots) for i in range(NVECS)]
-
+        
         self.instrs.extend(self._pack_slots(init_slots))
         self.add("load", ("const", batch_counter, 0))
         
@@ -228,13 +233,13 @@
         
         ptrs_i = [self.alloc_scratch() for _ in range(NVECS)]
         ptrs_v = [self.alloc_scratch() for _ in range(NVECS)]
-
-        # Compute pointer offsets independently (no chain dependency)
-        ptr_offsets = [self.scratch_const(i * VLEN, slots=init_slots) for i in range(NVECS)]
+        
         setup_ptrs = []
-        for i in range(NVECS):
-            setup_ptrs.append(("alu", ("+", ptrs_i[i], ptr_idx, ptr_offsets[i])))
-            setup_ptrs.append(("alu", ("+", ptrs_v[i], ptr_val, ptr_offsets[i])))
+        setup_ptrs.append(("alu", ("+", ptrs_i[0], ptr_idx, zero_const)))
+        setup_ptrs.append(("alu", ("+", ptrs_v[0], ptr_val, zero_const)))
+        for i in range(1, NVECS):
+             setup_ptrs.append(("alu", ("+", ptrs_i[i], ptrs_i[i-1], vlen_const)))
+             setup_ptrs.append(("alu", ("+", ptrs_v[i], ptrs_v[i-1], vlen_const)))
         self.instrs.extend(self._pack_slots(setup_ptrs))
         
         load_ops = []
@@ -332,39 +337,55 @@
         self.instrs.extend(self._pack_slots(r0_comp))
         
         # --- Round 1 Optimization (Select 1 or 2) ---
-        # Idx is 1 or 2. Load Tree[1], Tree[2] in parallel.
+        # Idx is 1 or 2. Load Tree[1], Tree[2].
+        r1_setup = []
         node1_scalar = self.alloc_scratch("n1_s")
         node2_scalar = self.alloc_scratch("n2_s")
-        tmp_ptr1 = self.alloc_scratch("tmp_ptr1")
-        tmp_ptr2 = self.alloc_scratch("tmp_ptr2")
-        t1_vec = self.alloc_scratch("t1_vec", VLEN)
-        t2_vec = self.alloc_scratch("t2_vec", VLEN)
-        two_vec = self.alloc_scratch("two_vec", VLEN)
-
-        r1_setup = []
+        tmp_ptr = self.alloc_scratch("tmp_ptr")
         const_1 = self.scratch_const(1, slots=r1_setup)
         const_2 = self.scratch_const(2, slots=r1_setup)
-        # Compute both addresses, then load both in parallel (2 load slots available)
-        r1_setup.append(("alu", ("+", tmp_ptr1, forest_values_p, const_1)))
-        r1_setup.append(("alu", ("+", tmp_ptr2, forest_values_p, const_2)))
-        r1_setup.append(("load", ("load", node1_scalar, tmp_ptr1)))
-        r1_setup.append(("load", ("load", node2_scalar, tmp_ptr2)))
-        # Broadcast all three vectors together
-        r1_setup.append(("valu", ("vbroadcast", t1_vec, node1_scalar)))
-        r1_setup.append(("valu", ("vbroadcast", t2_vec, node2_scalar)))
-        r1_setup.append(("valu", ("vbroadcast", two_vec, const_2)))
-        self.instrs.extend(self._pack_slots(r1_setup))
         
-        # Per vector select - batch all comparisons then all vselects
-        select_conds = [self.alloc_scratch(f"sel_cond_{v}", VLEN) for v in range(NVECS)]
-        sel_ops = []
-        # Batch all comparisons (can run 6 valu ops per cycle)
+        # Load T1 (at base + 1)
+        r1_setup.append(("alu", ("+", tmp_ptr, forest_values_p, const_1)))
+        r1_setup.append(("load", ("load", node1_scalar, tmp_ptr)))
+        # Load T2 (at base + 2)
+        # Note: We can schedule separate ALU/Load if needed, packer handles it.
+        # But we need tmp_ptr update. 
+        # Pack splits if dependency.
+        self.instrs.extend(self._pack_slots(r1_setup))
+        
+        r1_load2 = []
+        r1_load2.append(("alu", ("+", tmp_ptr, forest_values_p, const_2)))
+        r1_load2.append(("load", ("load", node2_scalar, tmp_ptr)))
+        self.instrs.extend(self._pack_slots(r1_load2))
+        
+        # Select
+        r1_select = []
+        two_vec = self.alloc_scratch("two_vec", VLEN)
+        r1_select.append(("valu", ("vbroadcast", two_vec, const_2)))
+        self.instrs.extend(self._pack_slots(r1_select))
+        
+        r1_ops = []
+        # Reuse select_temps for broadcast vectors
+        # T1_vec = sel_0_0 (shared across vectors? No, need per-vector destination? No, T1_vec can be shared source for vselect)
+        # Actually vselect needs vector operands.
+        # We can allocate T1_vec, T2_vec once.
+        t1_vec = self.alloc_scratch("t1_vec", VLEN)
+        t2_vec = self.alloc_scratch("t2_vec", VLEN)
+        
+        r1_ops.append(("valu", ("vbroadcast", t1_vec, node1_scalar)))
+        r1_ops.append(("valu", ("vbroadcast", t2_vec, node2_scalar)))
+        self.instrs.extend(self._pack_slots(r1_ops))
+        
+        # Per vector select
+        # Need to allocate select_temps_sets
+        select_temps_sets = [[self.alloc_scratch(f"sel_cond_{v}_{i}") for i in range(1)] for v in range(NVECS)]
         for v in range(NVECS):
-            sel_ops.append(("valu", ("==", select_conds[v], idx_vecs[v], two_vec)))
-        # Batch all vselects (1 flow op per cycle, but packer handles it)
-        for v in range(NVECS):
-            sel_ops.append(("flow", ("vselect", node_vecs[v], select_conds[v], t2_vec, t1_vec)))
-        self.instrs.extend(self._pack_slots(sel_ops))
+            sel_ops = []
+            cond = select_temps_sets[v][0]
+            sel_ops.append(("valu", ("==", cond, idx_vecs[v], two_vec)))
+            sel_ops.append(("flow", ("vselect", node_vecs[v], cond, t2_vec, t1_vec)))
+            self.instrs.extend(self._pack_slots(sel_ops))
             
         # Compute R1
         r1_comp = []
@@ -378,17 +399,102 @@
 
         # --- Pipeline R2..15 ---
         
-        # Prime R2: Gather A only.
-        emit_pipelined_step(gatherA=True, hashA=False, gatherB=False, hashB=False)
+        # Helper to get subset of vectors
+        def get_grps(grp_indices):
+            g_idx = []
+            g_val = []
+            g_addr = []
+            g_node = []
+            g_temps = [] 
+            
+            for g in grp_indices:
+                base = g * 2
+                g_idx.extend(idx_vecs[base:base+2])
+                g_val.extend(val_vecs[base:base+2])
+                g_addr.extend(addr_vecs[base:base+2])
+                g_node.extend(node_vecs[base:base+2])
+                
+                # Temps mapping
+                if g == 0:
+                     g_temps.append((temps_A[0], temps_A[4]))
+                     g_temps.append((temps_A[1], temps_A[5]))
+                elif g == 1:
+                     g_temps.append((temps_A[2], temps_A[6]))
+                     g_temps.append((temps_A[3], temps_A[7]))
+                elif g == 2:
+                     g_temps.append((temps_B[0], temps_B[4]))
+                     g_temps.append((temps_B[1], temps_B[5]))
+                elif g == 3:
+                     g_temps.append((temps_B[2], temps_B[6]))
+                     g_temps.append((temps_B[3], temps_B[7]))
+
+            final_temps = []
+            t_list = [x[0] for x in g_temps]
+            tt_list = [x[1] for x in g_temps]
+            final_temps = t_list + tt_list
+            
+            return g_idx, g_val, g_addr, g_node, final_temps
+            
+        def gen_index_update_opt(idx_subset, val_subset, hash_temps, node_nodes):
+            t0, t1, t2, t3, tt0, tt1, tt2, tt3 = hash_temps
+            ops = []
+            # Pipeline:
+            # 1. t = val & 1
+            # 2. t = t + 1
+            # 3. idx = idx * 2 + t (multiply_add)
+            # 4. wrap
+            
+            # 1. t = val & 1
+            for i in range(len(idx_subset)):
+                ops.append(("valu", ("&", [t0,t1,t2,t3][i], val_subset[i], one_vec)))
+                
+            # 2. t = t + 1
+            for i in range(len(idx_subset)):
+                ops.append(("valu", ("+", [t0,t1,t2,t3][i], [t0,t1,t2,t3][i], one_vec)))
+                
+            # 3. idx = idx * 2 + t
+            for i in range(len(idx_subset)):
+                ops.append(("valu", ("multiply_add", idx_subset[i], idx_subset[i], two_vec, [t0,t1,t2,t3][i])))
+                
+            # 4. Wrap: idx = idx * (idx < n_nodes)
+            for i in range(len(idx_subset)):
+                ops.append(("valu", ("<", [tt0,tt1,tt2,tt3][i], idx_subset[i], node_nodes)))
+            for i in range(len(idx_subset)):
+                ops.append(("valu", ("*", idx_subset[i], idx_subset[i], [tt0,tt1,tt2,tt3][i])))
+                
+            return ops
+
+        def emit_swizzled_step(gather_grps, hash_grps):
+            ops = []
+            
+            # Gather
+            if gather_grps:
+                g_idx, g_val, g_addr, g_node, _ = get_grps(gather_grps)
+                ops.extend(gen_gather(g_idx, g_addr, g_node))
+            
+            # Hash
+            if hash_grps:
+                h_idx, h_val, h_addr, h_node, h_temps = get_grps(hash_grps)
+                ops.extend(gen_xor(h_val, h_node))
+                ops.extend(gen_hash(h_val, h_temps))
+                ops.extend(gen_index_update_opt(h_idx, h_val, h_temps, n_nodes_vec))
+                
+            self.instrs.extend(self._pack_slots(ops))
         
+        # Prime R2: Gather A(0), C(2)
+        emit_swizzled_step(gather_grps=[0, 2], hash_grps=[])
+        
         # Loop R2..
         for r in range(2, rounds):
-            # Step 1: Hash A(r) | Gather B(r)
-            emit_pipelined_step(gatherA=False, hashA=True, gatherB=True, hashB=False)
+            # Step 1: Hash A, C (r). Gather B, D (r).
+            # Groups 0, 2 Hash. Groups 1, 3 Gather.
+            emit_swizzled_step(gather_grps=[1, 3], hash_grps=[0, 2])
             
-            # Step 2: Gather A(r+1) | Hash B(r)
-            next_gather = (r + 1 < rounds)
-            emit_pipelined_step(gatherA=next_gather, hashA=False, gatherB=False, hashB=True)
+            # Step 2: Hash B, D (r). Gather A, C (r+1).
+            next_gather = []
+            if r + 1 < rounds:
+                next_gather = [0, 2]
+            emit_swizzled_step(gather_grps=next_gather, hash_grps=[1, 3])
              
         # Store
         store_ops = []
