diff --git a/perf_takehome.py b/perf_takehome.py
index 4188190..43e6f4f 100644
--- a/perf_takehome.py
+++ b/perf_takehome.py
@@ -1,22 +1,7 @@
-"""
-# Anthropic's Original Performance Engineering Take-home (Release version)
-
-Copyright Anthropic PBC 2026. Permission is granted to modify and use, but not
-to publish or redistribute your solutions so it's hard to find spoilers.
-
-# Task
-
-- Optimize the kernel (in KernelBuilder.build_kernel) as much as possible in the
-  available time, as measured by test_kernel_cycles on a frozen separate copy
-  of the simulator.
-
-We recommend you look through problem.py next.
-"""
 
 from collections import defaultdict
 import random
 import unittest
-
 from problem import (
     Engine,
     DebugInfo,
@@ -33,7 +18,6 @@ from problem import (
     reference_kernel2,
 )
 
-
 class KernelBuilder:
     def __init__(self):
         self.instrs = []
@@ -45,13 +29,6 @@ class KernelBuilder:
     def debug_info(self):
         return DebugInfo(scratch_map=self.scratch_debug)
 
-    def build(self, slots: list[tuple[Engine, tuple]], vliw: bool = False):
-        # Simple slot packing that just uses one slot per instruction bundle
-        instrs = []
-        for engine, slot in slots:
-            instrs.append({engine: [slot]})
-        return instrs
-
     def add(self, engine, slot):
         self.instrs.append({engine: [slot]})
 
@@ -61,114 +38,650 @@ class KernelBuilder:
             self.scratch[name] = addr
             self.scratch_debug[addr] = (name, length)
         self.scratch_ptr += length
-        assert self.scratch_ptr <= SCRATCH_SIZE, "Out of scratch space"
+        assert self.scratch_ptr <= SCRATCH_SIZE, f"Out of scratch space: {self.scratch_ptr}"
         return addr
 
-    def scratch_const(self, val, name=None):
+    def scratch_const(self, val, name=None, slots=None):
         if val not in self.const_map:
-            addr = self.alloc_scratch(name)
-            self.add("load", ("const", addr, val))
+            addr = self.alloc_scratch(f"c_{val}" if name is None else name)
+            if slots is None:
+                self.add("load", ("const", addr, val))
+            else:
+                slots.append(("load", ("const", addr, val)))
             self.const_map[val] = addr
         return self.const_map[val]
 
-    def build_hash(self, val_hash_addr, tmp1, tmp2, round, i):
-        slots = []
-
-        for hi, (op1, val1, op2, op3, val3) in enumerate(HASH_STAGES):
-            slots.append(("alu", (op1, tmp1, val_hash_addr, self.scratch_const(val1))))
-            slots.append(("alu", (op3, tmp2, val_hash_addr, self.scratch_const(val3))))
-            slots.append(("alu", (op2, val_hash_addr, tmp1, tmp2)))
-            slots.append(("debug", ("compare", val_hash_addr, (round, i, "hash_stage", hi))))
-
-        return slots
-
-    def build_kernel(
-        self, forest_height: int, n_nodes: int, batch_size: int, rounds: int
-    ):
-        """
-        Like reference_kernel2 but building actual instructions.
-        Scalar implementation using only scalar ALU and load/store.
-        """
-        tmp1 = self.alloc_scratch("tmp1")
-        tmp2 = self.alloc_scratch("tmp2")
-        tmp3 = self.alloc_scratch("tmp3")
-        # Scratch space addresses
-        init_vars = [
-            "rounds",
-            "n_nodes",
-            "batch_size",
-            "forest_height",
-            "forest_values_p",
-            "inp_indices_p",
-            "inp_values_p",
+    def _slot_rw(self, engine, slot):
+        reads = set()
+        writes = set()
+        if engine == "alu":
+            if len(slot) == 4:
+                _op, dest, a1, a2 = slot
+                reads.update([a1, a2])
+                writes.add(dest)
+            elif len(slot) == 3:
+                _op, dest, a1 = slot
+                reads.add(a1)
+                writes.add(dest)
+        elif engine == "valu":
+            match slot:
+                case ("vbroadcast", dest, src):
+                    reads.add(src)
+                    for i in range(VLEN): writes.add(dest + i)
+                case ("multiply_add", dest, a, b, c):
+                    for i in range(VLEN):
+                        reads.add(a + i)
+                        reads.add(b + i)
+                        reads.add(c + i)
+                        writes.add(dest + i)
+                case (_op, dest, a1, a2):
+                    for i in range(VLEN):
+                        reads.add(a1 + i)
+                        reads.add(a2 + i)
+                        writes.add(dest + i)
+        elif engine == "load":
+            match slot:
+                case ("load", dest, addr):
+                    reads.add(addr)
+                    writes.add(dest)
+                case ("load_offset", dest, addr, offset):
+                    reads.add(addr + offset)
+                    writes.add(dest + offset)
+                case ("vload", dest, addr):
+                    reads.add(addr)
+                    for i in range(VLEN): writes.add(dest + i)
+                case ("const", dest, _val):
+                    writes.add(dest)
+        elif engine == "store":
+            match slot:
+                case ("store", addr, src):
+                    reads.update([addr, src])
+                case ("vstore", addr, src):
+                    reads.add(addr)
+                    for i in range(VLEN): reads.add(src + i)
+        elif engine == "flow":
+            match slot:
+                case ("select", dest, cond, a, b):
+                    reads.update([cond, a, b])
+                    writes.add(dest)
+                case ("add_imm", dest, a, _imm):
+                    reads.add(a)
+                    writes.add(dest)
+                case ("vselect", dest, cond, a, b):
+                    for i in range(VLEN):
+                        reads.add(cond + i)
+                        reads.add(a + i)
+                        reads.add(b + i)
+                        writes.add(dest + i)
+                case ("cond_jump", cond, addr):
+                    reads.update([cond, addr])
+                case ("cond_jump_rel", cond, _offset):
+                    reads.add(cond)
+                case ("jump", addr):
+                    reads.add(addr)
+                case ("jump_indirect", addr):
+                    reads.add(addr)
+                case ("coreid", dest):
+                    writes.add(dest)
+                case ("trace_write", val):
+                    reads.add(val)
+                case ("pause",):
+                    pass
+                case ("halt",):
+                    pass
+        return reads, writes
+
+    def _pack_slots(self, slots: list[tuple[Engine, tuple]]):
+        instrs = []
+        bundle = {}
+        bundle_writes = set()
+        engine_counts = defaultdict(int)
+
+        def flush_bundle():
+            nonlocal bundle, bundle_writes, engine_counts
+            if bundle:
+                instrs.append(bundle)
+                bundle = {}
+                bundle_writes = set()
+                engine_counts = defaultdict(int)
+
+        for engine, slot in slots:
+            reads, writes = self._slot_rw(engine, slot)
+            if (
+                engine_counts[engine] >= SLOT_LIMITS[engine]
+                or reads & bundle_writes
+                or writes & bundle_writes
+            ):
+                flush_bundle()
+            bundle.setdefault(engine, []).append(slot)
+            engine_counts[engine] += 1
+            bundle_writes.update(writes)
+
+        flush_bundle()
+        return instrs
+
+    def build_kernel(self, forest_height: int, n_nodes: int, batch_size: int, rounds: int):
+        init_slots = []
+        NVECS = 8 
+        
+        # Constants
+        zero_const = self.scratch_const(0, "zero", init_slots)
+        one_const = self.scratch_const(1, "one", init_slots)
+        vlen_const = self.scratch_const(VLEN, "vlen", init_slots)
+        vlen8_const = self.scratch_const(VLEN * NVECS, "vlen8", init_slots)
+        n_nodes_const = self.scratch_const(n_nodes, "n_nodes", init_slots)
+        batch_end = self.scratch_const(batch_size, "batch_end", init_slots)
+        
+        forest_values_p = self.scratch_const(7, "forest_values_p", init_slots)
+        inp_indices_p = self.scratch_const(7 + n_nodes, "inp_indices_p", init_slots)
+        inp_values_p = self.scratch_const(7 + n_nodes + batch_size, "inp_values_p", init_slots)
+
+        batch_counter = self.alloc_scratch("batch_counter")
+        
+        # Vectors
+        idx_vecs = [self.alloc_scratch(f"idx_vec{i}", VLEN) for i in range(NVECS)]
+        val_vecs = [self.alloc_scratch(f"val_vec{i}", VLEN) for i in range(NVECS)]
+        addr_vecs = [self.alloc_scratch(f"addr_vec{i}", VLEN) for i in range(NVECS)]
+        node_vecs = [self.alloc_scratch(f"node_vec{i}", VLEN) for i in range(NVECS)]
+        
+        # Hash Temps
+        temps_A = [self.alloc_scratch(f"ha_{i}", VLEN) for i in range(8)]
+        temps_B = [self.alloc_scratch(f"hb_{i}", VLEN) for i in range(8)]
+        
+        # Constants
+        one_vec = self.alloc_scratch("one_vec", VLEN)
+        two_const_init = self.scratch_const(2, "two_init", init_slots)
+        idx_two_vec = self.alloc_scratch("idx_two_vec", VLEN)
+        n_nodes_vec = self.alloc_scratch("n_nodes_vec", VLEN)
+        forest_base_vec = self.alloc_scratch("forest_base_vec", VLEN)
+
+        init_slots.append(("valu", ("vbroadcast", one_vec, one_const)))
+        init_slots.append(("valu", ("vbroadcast", idx_two_vec, two_const_init)))
+        init_slots.append(("valu", ("vbroadcast", n_nodes_vec, n_nodes_const)))
+        init_slots.append(("valu", ("vbroadcast", forest_base_vec, forest_values_p)))
+        
+        # Hash Constants
+        hash_const_vecs = []
+        for op1, val1, op2, op3, val3 in HASH_STAGES:
+            c1 = self.scratch_const(val1, slots=init_slots)
+            c3 = self.scratch_const(val3, slots=init_slots)
+            v1 = self.alloc_scratch(length=VLEN)
+            v3 = self.alloc_scratch(length=VLEN)
+            init_slots.append(("valu", ("vbroadcast", v1, c1)))
+            init_slots.append(("valu", ("vbroadcast", v3, c3)))
+            hash_const_vecs.append((op1, op2, op3, v1, v3))
+
+        # Pointer offset constants for independent pointer setup
+        ptr_offsets = [self.scratch_const(i * VLEN, slots=init_slots) for i in range(NVECS)]
+
+        # Pre-compute base addresses with offsets (static, computed once)
+        idx_base_offsets = [self.alloc_scratch(f"idx_base_{i}") for i in range(NVECS)]
+        val_base_offsets = [self.alloc_scratch(f"val_base_{i}") for i in range(NVECS)]
+        for i in range(NVECS):
+            init_slots.append(("alu", ("+", idx_base_offsets[i], inp_indices_p, ptr_offsets[i])))
+            init_slots.append(("alu", ("+", val_base_offsets[i], inp_values_p, ptr_offsets[i])))
+
+        self.instrs.extend(self._pack_slots(init_slots))
+        self.add("load", ("const", batch_counter, 0))
+
+        # BATCH LOOP
+        batch_loop_start = len(self.instrs)
+
+        ptrs_i = [self.alloc_scratch() for _ in range(NVECS)]
+        ptrs_v = [self.alloc_scratch() for _ in range(NVECS)]
+
+        # Compute all pointers: ptrs[i] = base_offset[i] + batch_counter (all independent)
+        setup_ptrs = []
+        for i in range(NVECS):
+            setup_ptrs.append(("alu", ("+", ptrs_i[i], idx_base_offsets[i], batch_counter)))
+            setup_ptrs.append(("alu", ("+", ptrs_v[i], val_base_offsets[i], batch_counter)))
+        self.instrs.extend(self._pack_slots(setup_ptrs))
+
+        # Only load val_vecs; idx_vecs don't need loading since all indices start at 0
+        # and round 0's index update doesn't read idx (it computes idx = (val & 1) + 1)
+        load_ops = []
+        for i in range(NVECS):
+            load_ops.append(("load", ("vload", val_vecs[i], ptrs_v[i])))
+        self.instrs.extend(self._pack_slots(load_ops))
+        
+        # Logic Helpers (Same as before)
+        def gen_hash(vec_subset, hash_temps):
+            t0, t1, t2, t3, tt0, tt1, tt2, tt3 = hash_temps
+            ops = []
+            val_t_tt = zip(vec_subset, [t0, t1, t2, t3], [tt0, tt1, tt2, tt3])
+            val_t_tt_list = list(val_t_tt)
+            for op1, op2, op3, c1, c3 in hash_const_vecs:
+                 for (val, t, tt) in val_t_tt_list:
+                     ops.append(("valu", (op1, t, val, c1)))
+                     ops.append(("valu", (op3, tt, val, c3)))
+                 for (val, t, tt) in val_t_tt_list:
+                     ops.append(("valu", (op2, val, t, tt)))
+            return ops
+        
+        def gen_index_update(idx_subset, val_subset, hash_temps, node_nodes, with_wrap=True):
+            t0, t1, t2, t3, tt0, tt1, tt2, tt3 = hash_temps
+            ops = []
+            # New formulation: idx = idx*2 + 1 + (val & 1)
+            # Step 1: base = idx*2+1 (multiply_add) and t = val&1 (AND) - INDEPENDENT!
+            # Step 2: idx = base + t (ADD)
+            for i in range(4):
+                ops.append(("valu", ("multiply_add", [tt0,tt1,tt2,tt3][i], idx_subset[i], idx_two_vec, one_vec)))  # base = idx*2+1
+            for i in range(4):
+                ops.append(("valu", ("&", [t0,t1,t2,t3][i], val_subset[i], one_vec)))  # t = val&1
+            # ADD: idx = base + t
+            for i in range(4):
+                ops.append(("valu", ("+", idx_subset[i], [tt0,tt1,tt2,tt3][i], [t0,t1,t2,t3][i])))
+            # Wrap: idx = idx * (idx < n_nodes)
+            if with_wrap:
+                for i in range(4):
+                    ops.append(("valu", ("<", [t0,t1,t2,t3][i], idx_subset[i], node_nodes)))
+                for i in range(4):
+                    ops.append(("valu", ("*", idx_subset[i], idx_subset[i], [t0,t1,t2,t3][i])))
+            return ops
+
+        def gen_index_update_both(idx_A, val_A, temps_A_arg, idx_B, val_B, temps_B_arg, node_nodes, with_wrap=True):
+            """Interleaved idx_update for both A and B groups to improve packing.
+            Uses new formulation: idx = idx*2+1 + (val&1) where multiply_add and AND are independent."""
+            tA = temps_A_arg[:4]
+            ttA = temps_A_arg[4:]
+            tB = temps_B_arg[:4]
+            ttB = temps_B_arg[4:]
+            ops = []
+            # Step 1: multiply_adds (base = idx*2+1) and ANDs (t = val&1) - all independent!
+            for i in range(4):
+                ops.append(("valu", ("multiply_add", ttA[i], idx_A[i], idx_two_vec, one_vec)))  # A's base
+                ops.append(("valu", ("multiply_add", ttB[i], idx_B[i], idx_two_vec, one_vec)))  # B's base
+            for i in range(4):
+                ops.append(("valu", ("&", tA[i], val_A[i], one_vec)))  # A's t
+                ops.append(("valu", ("&", tB[i], val_B[i], one_vec)))  # B's t
+            # Step 2: ADDs - interleave A and B (these depend on step 1)
+            for i in range(4):
+                ops.append(("valu", ("+", idx_A[i], ttA[i], tA[i])))
+                ops.append(("valu", ("+", idx_B[i], ttB[i], tB[i])))
+            # Wrap
+            if with_wrap:
+                for i in range(4):
+                    ops.append(("valu", ("<", tA[i], idx_A[i], node_nodes)))
+                    ops.append(("valu", ("<", tB[i], idx_B[i], node_nodes)))
+                for i in range(4):
+                    ops.append(("valu", ("*", idx_A[i], idx_A[i], tA[i])))
+                    ops.append(("valu", ("*", idx_B[i], idx_B[i], tB[i])))
+            return ops
+
+        def gen_gather(idx_subset, addr_subset, node_subset):
+            ops = []
+            for i in range(4):
+                ops.append(("valu", ("+", addr_subset[i], idx_subset[i], forest_base_vec)))
+            for i in range(4):
+                dest = node_subset[i]
+                addr = addr_subset[i]
+                for vi in range(VLEN):
+                     ops.append(("load", ("load_offset", dest, addr, vi)))
+            return ops
+            
+        def gen_xor(val_subset, node_subset):
+             ops = []
+             for i in range(4):
+                 ops.append(("valu", ("^", val_subset[i], val_subset[i], node_subset[i])))
+             return ops
+             
+        def emit_pipelined_step(gatherA, hashA, gatherB, hashB, with_wrap=True):
+            ops = []
+
+            # Key insight: loads (2/cycle) and valu (6/cycle) can run in parallel
+            # We need to interleave them AFTER addr calculation (which must precede loads)
+
+            if gatherA and gatherB:
+                # Both groups gathering - interleave A and B loads
+                ops_a = gen_gather(idx_vecs[:4], addr_vecs[:4], node_vecs[:4])
+                ops_b = gen_gather(idx_vecs[4:], addr_vecs[4:], node_vecs[4:])
+                ops.extend(ops_a[:4])  # addr A (4 valu)
+                ops.extend(ops_b[:4])  # addr B (4 valu)
+                for i in range(32):
+                    ops.append(ops_a[4 + i])
+                    ops.append(ops_b[4 + i])
+            elif gatherA and hashB:
+                # Gather A, hash B - interleave A's loads with B's hash
+                # Key insight: XOR writes val, hash reads val - dependency!
+                # Emit XOR first with addr calc, then interleave hash+idx with loads
+                gather_ops = gen_gather(idx_vecs[:4], addr_vecs[:4], node_vecs[:4])
+                xor_ops = gen_xor(val_vecs[4:], node_vecs[4:])
+                hash_ops = gen_hash(val_vecs[4:], temps_B)
+                hash_ops.extend(gen_index_update(idx_vecs[4:], val_vecs[4:], temps_B, n_nodes_vec, with_wrap))
+                # Addr calc + XOR first (XOR depends on node_vecs, which are ready)
+                ops.extend(gather_ops[:4])  # 4 valu (addr calc)
+                ops.extend(xor_ops)  # 4 valu (XOR)
+                # Interleave: 2 loads per 6 valu (hash + idx_update only, no XOR)
+                load_ops = gather_ops[4:]  # 32 loads
+                load_idx = 0
+                hash_idx = 0
+                while load_idx < len(load_ops) or hash_idx < len(hash_ops):
+                    for _ in range(2):
+                        if load_idx < len(load_ops):
+                            ops.append(load_ops[load_idx])
+                            load_idx += 1
+                    for _ in range(6):
+                        if hash_idx < len(hash_ops):
+                            ops.append(hash_ops[hash_idx])
+                            hash_idx += 1
+            elif gatherB and hashA:
+                # Gather B, hash A - interleave B's loads with A's hash
+                # Same fix: emit XOR first, then interleave hash+idx with loads
+                gather_ops = gen_gather(idx_vecs[4:], addr_vecs[4:], node_vecs[4:])
+                xor_ops = gen_xor(val_vecs[:4], node_vecs[:4])
+                hash_ops = gen_hash(val_vecs[:4], temps_A)
+                hash_ops.extend(gen_index_update(idx_vecs[:4], val_vecs[:4], temps_A, n_nodes_vec, with_wrap))
+                # Addr calc + XOR first
+                ops.extend(gather_ops[:4])  # 4 valu (addr calc)
+                ops.extend(xor_ops)  # 4 valu (XOR)
+                # Interleave: 2 loads per 6 valu (hash + idx_update only)
+                load_ops = gather_ops[4:]  # 32 loads
+                load_idx = 0
+                hash_idx = 0
+                while load_idx < len(load_ops) or hash_idx < len(hash_ops):
+                    for _ in range(2):
+                        if load_idx < len(load_ops):
+                            ops.append(load_ops[load_idx])
+                            load_idx += 1
+                    for _ in range(6):
+                        if hash_idx < len(hash_ops):
+                            ops.append(hash_ops[hash_idx])
+                            hash_idx += 1
+            elif gatherA:
+                ops.extend(gen_gather(idx_vecs[:4], addr_vecs[:4], node_vecs[:4]))
+            elif gatherB:
+                ops.extend(gen_gather(idx_vecs[4:], addr_vecs[4:], node_vecs[4:]))
+            elif hashA and hashB:
+                ops.extend(gen_xor(val_vecs[:4], node_vecs[:4]))
+                ops.extend(gen_xor(val_vecs[4:], node_vecs[4:]))
+                for op1, op2, op3, c1, c3 in hash_const_vecs:
+                    for i in range(4):
+                        ops.append(("valu", (op1, temps_A[i], val_vecs[i], c1)))
+                        ops.append(("valu", (op3, temps_A[4+i], val_vecs[i], c3)))
+                        ops.append(("valu", (op1, temps_B[i], val_vecs[4+i], c1)))
+                        ops.append(("valu", (op3, temps_B[4+i], val_vecs[4+i], c3)))
+                    for i in range(4):
+                        ops.append(("valu", (op2, val_vecs[i], temps_A[i], temps_A[4+i])))
+                        ops.append(("valu", (op2, val_vecs[4+i], temps_B[i], temps_B[4+i])))
+                ops.extend(gen_index_update_both(idx_vecs[:4], val_vecs[:4], temps_A,
+                                                idx_vecs[4:], val_vecs[4:], temps_B, n_nodes_vec, with_wrap))
+            elif hashA:
+                ops.extend(gen_xor(val_vecs[:4], node_vecs[:4]))
+                ops.extend(gen_hash(val_vecs[:4], temps_A))
+                ops.extend(gen_index_update(idx_vecs[:4], val_vecs[:4], temps_A, n_nodes_vec, with_wrap))
+            elif hashB:
+                ops.extend(gen_xor(val_vecs[4:], node_vecs[4:]))
+                ops.extend(gen_hash(val_vecs[4:], temps_B))
+                ops.extend(gen_index_update(idx_vecs[4:], val_vecs[4:], temps_B, n_nodes_vec, with_wrap))
+
+            self.instrs.extend(self._pack_slots(ops))
+
+        # EXECUTE ROUNDS
+
+        # --- Rounds 0/1/2 Prefetch Optimization ---
+        # Load tree[0..6] early to overlap with R0's hash computation
+        # This hides the load latency for R1 and R2
+
+        # Define gen_index_update_r0 helper first
+        def gen_index_update_r0(idx_subset, val_subset, hash_temps):
+            t0, t1, t2, t3, tt0, tt1, tt2, tt3 = hash_temps
+            ops = []
+            # idx = (val & 1) + 1 (no multiply needed since idx=0)
+            for i in range(4):
+                ops.append(("valu", ("&", idx_subset[i], val_subset[i], one_vec)))
+            for i in range(4):
+                ops.append(("valu", ("+", idx_subset[i], idx_subset[i], one_vec)))
+            return ops
+
+        # Allocate all scalars for tree nodes 0-6
+        r0_node_scalar = self.alloc_scratch("r0_scalar")
+        node1_scalar = self.alloc_scratch("n1_s")
+        node2_scalar = self.alloc_scratch("n2_s")
+        node3_s = self.alloc_scratch("n3_s")
+        node4_s = self.alloc_scratch("n4_s")
+        node5_s = self.alloc_scratch("n5_s")
+        node6_s = self.alloc_scratch("n6_s")
+
+        # Address pointers for tree[1..6]
+        tmp_ptr1 = self.alloc_scratch("tmp_ptr1")
+        tmp_ptr2 = self.alloc_scratch("tmp_ptr2")
+        r2_ptr1 = self.alloc_scratch("r2_ptr1")
+        r2_ptr2 = self.alloc_scratch("r2_ptr2")
+        r2_ptr3 = self.alloc_scratch("r2_ptr3")
+        r2_ptr4 = self.alloc_scratch("r2_ptr4")
+
+        # Vectors for R1 (linear interpolation uses fewer vectors)
+        t1_vec = self.alloc_scratch("t1_vec", VLEN)
+        t2_vec = self.alloc_scratch("t2_vec", VLEN)
+        two_vec = self.alloc_scratch("two_vec", VLEN)  # Reused as diff = t2 - t1
+        r1_tmp = [self.alloc_scratch(f"r1_tmp_{v}", VLEN) for v in range(NVECS)]
+
+        # Vectors for R2 (bit-based selection uses fewer vectors)
+        t3_vec = self.alloc_scratch("t3_vec", VLEN)
+        t4_vec = self.alloc_scratch("t4_vec", VLEN)
+        t5_vec = self.alloc_scratch("t5_vec", VLEN)
+        t6_vec = self.alloc_scratch("t6_vec", VLEN)
+        three_vec = self.alloc_scratch("three_vec", VLEN)  # For idx - 3
+        four_vec = self.alloc_scratch("four_vec", VLEN)    # Reused as d34 = t4 - t3
+        five_vec = self.alloc_scratch("five_vec", VLEN)    # Reused as d56 = t6 - t5
+        r2_cond3 = [self.alloc_scratch(f"r2_c3_{i}", VLEN) for i in range(NVECS)]  # sel0
+        r2_cond4 = [self.alloc_scratch(f"r2_c4_{i}", VLEN) for i in range(NVECS)]  # sel1
+        r2_cond5 = [self.alloc_scratch(f"r2_c5_{i}", VLEN) for i in range(NVECS)]  # low
+        r2_cond6 = [self.alloc_scratch(f"r2_c6_{i}", VLEN) for i in range(NVECS)]  # high
+        r2_tmp = [self.alloc_scratch(f"r2_tmp_{i}", VLEN) for i in range(NVECS)]
+
+        # --- Round 0 with R1/R2 prefetch ---
+        r0_all = []
+
+        # Constants for addresses
+        const_1 = self.scratch_const(1, slots=r0_all)
+        const_2 = self.scratch_const(2, slots=r0_all)
+        const_3 = self.scratch_const(3, slots=r0_all)
+        const_4 = self.scratch_const(4, slots=r0_all)
+        const_5 = self.scratch_const(5, slots=r0_all)
+        const_6 = self.scratch_const(6, slots=r0_all)
+
+        # Compute all addresses (ALU - independent)
+        r0_all.append(("alu", ("+", tmp_ptr1, forest_values_p, const_1)))
+        r0_all.append(("alu", ("+", tmp_ptr2, forest_values_p, const_2)))
+        r0_all.append(("alu", ("+", r2_ptr1, forest_values_p, const_3)))
+        r0_all.append(("alu", ("+", r2_ptr2, forest_values_p, const_4)))
+        r0_all.append(("alu", ("+", r2_ptr3, forest_values_p, const_5)))
+        r0_all.append(("alu", ("+", r2_ptr4, forest_values_p, const_6)))
+
+        # Load tree[0] (R0 needs this)
+        r0_all.append(("load", ("load", r0_node_scalar, forest_values_p)))
+
+        # Broadcasts for R0 (depend on tree[0] load)
+        for v in range(NVECS):
+            r0_all.append(("valu", ("vbroadcast", node_vecs[v], r0_node_scalar)))
+
+        # Now interleave: R0's XOR/hash/idx_update with loads for R1/R2
+        r0_compute = []
+        r0_compute.extend(gen_xor(val_vecs[:4], node_vecs[:4]))
+        r0_compute.extend(gen_xor(val_vecs[4:], node_vecs[4:]))
+        r0_compute.extend(gen_hash(val_vecs[:4], temps_A))
+        r0_compute.extend(gen_hash(val_vecs[4:], temps_B))
+        r0_compute.extend(gen_index_update_r0(idx_vecs[:4], val_vecs[:4], temps_A))
+        r0_compute.extend(gen_index_update_r0(idx_vecs[4:], val_vecs[4:], temps_B))
+
+        # Loads for R1/R2 (6 loads total)
+        prefetch_loads = [
+            ("load", ("load", node1_scalar, tmp_ptr1)),
+            ("load", ("load", node2_scalar, tmp_ptr2)),
+            ("load", ("load", node3_s, r2_ptr1)),
+            ("load", ("load", node4_s, r2_ptr2)),
+            ("load", ("load", node5_s, r2_ptr3)),
+            ("load", ("load", node6_s, r2_ptr4)),
         ]
-        for v in init_vars:
-            self.alloc_scratch(v, 1)
-        for i, v in enumerate(init_vars):
-            self.add("load", ("const", tmp1, i))
-            self.add("load", ("load", self.scratch[v], tmp1))
-
-        zero_const = self.scratch_const(0)
-        one_const = self.scratch_const(1)
-        two_const = self.scratch_const(2)
-
-        # Pause instructions are matched up with yield statements in the reference
-        # kernel to let you debug at intermediate steps. The testing harness in this
-        # file requires these match up to the reference kernel's yields, but the
-        # submission harness ignores them.
+
+        # Interleave: 2 loads per 6 valu
+        load_idx = 0
+        compute_idx = 0
+        while load_idx < len(prefetch_loads) or compute_idx < len(r0_compute):
+            for _ in range(2):
+                if load_idx < len(prefetch_loads):
+                    r0_all.append(prefetch_loads[load_idx])
+                    load_idx += 1
+            for _ in range(6):
+                if compute_idx < len(r0_compute):
+                    r0_all.append(r0_compute[compute_idx])
+                    compute_idx += 1
+
+        self.instrs.extend(self._pack_slots(r0_all))
+
+        # --- Round 1 (tree nodes already loaded) ---
+        # Linear interpolation: node = t1 + (t2 - t1) * (idx - 1)
+        # Since idx is 1 or 2, (idx-1) is 0 or 1, so this selects t1 or t2
+        r1_all = []
+        # Broadcasts (tree[1], tree[2] already loaded during R0)
+        r1_all.append(("valu", ("vbroadcast", t1_vec, node1_scalar)))
+        r1_all.append(("valu", ("vbroadcast", t2_vec, node2_scalar)))
+        # Compute diff = t2 - t1 (one-time)
+        r1_all.append(("valu", ("-", two_vec, t2_vec, t1_vec)))  # Reuse two_vec as diff
+        # For each vector: offset = idx - 1, then node = t1 + diff * offset
+        for v in range(NVECS):
+            r1_all.append(("valu", ("-", r1_tmp[v], idx_vecs[v], one_vec)))  # offset = idx - 1
+        for v in range(NVECS):
+            r1_all.append(("valu", ("multiply_add", node_vecs[v], two_vec, r1_tmp[v], t1_vec)))  # node = diff*offset + t1
+        # Compute R1 (no wrap needed - max idx after R1 is 6 < n_nodes)
+        r1_all.extend(gen_xor(val_vecs[:4], node_vecs[:4]))
+        r1_all.extend(gen_xor(val_vecs[4:], node_vecs[4:]))
+        r1_all.extend(gen_hash(val_vecs[:4], temps_A))
+        r1_all.extend(gen_hash(val_vecs[4:], temps_B))
+        r1_all.extend(gen_index_update(idx_vecs[:4], val_vecs[:4], temps_A, n_nodes_vec, with_wrap=False))
+        r1_all.extend(gen_index_update(idx_vecs[4:], val_vecs[4:], temps_B, n_nodes_vec, with_wrap=False))
+        self.instrs.extend(self._pack_slots(r1_all))
+
+        # --- Round 2 (tree nodes already loaded during R0) ---
+        # Bit-based selection: idx is 3,4,5,6
+        # diff = idx - 3 → 0,1,2,3
+        # sel0 = diff & 1 → selects within pair (0→3, 1→4) or (0→5, 1→6)
+        # sel1 = diff >> 1 → selects between pairs (0→low, 1→high)
+        # low = t3 + (t4 - t3) * sel0
+        # high = t5 + (t6 - t5) * sel0
+        # node = low + (high - low) * sel1
+        r2_all = []
+        # Broadcasts (tree[3-6] already loaded during R0)
+        r2_all.append(("valu", ("vbroadcast", t3_vec, node3_s)))
+        r2_all.append(("valu", ("vbroadcast", t4_vec, node4_s)))
+        r2_all.append(("valu", ("vbroadcast", t5_vec, node5_s)))
+        r2_all.append(("valu", ("vbroadcast", t6_vec, node6_s)))
+        r2_all.append(("valu", ("vbroadcast", three_vec, const_3)))  # For idx - 3
+        # Compute diffs (once)
+        r2_all.append(("valu", ("-", four_vec, t4_vec, t3_vec)))  # d34 = t4 - t3 (reuse four_vec)
+        r2_all.append(("valu", ("-", five_vec, t6_vec, t5_vec)))  # d56 = t6 - t5 (reuse five_vec)
+        # For each vector compute selection
+        for v in range(NVECS):
+            r2_all.append(("valu", ("-", r2_tmp[v], idx_vecs[v], three_vec)))  # diff = idx - 3
+        for v in range(NVECS):
+            r2_all.append(("valu", ("&", r2_cond3[v], r2_tmp[v], one_vec)))  # sel0 = diff & 1
+        for v in range(NVECS):
+            r2_all.append(("valu", (">>", r2_cond4[v], r2_tmp[v], one_vec)))  # sel1 = diff >> 1
+        # low = t3 + d34 * sel0
+        for v in range(NVECS):
+            r2_all.append(("valu", ("multiply_add", r2_cond5[v], four_vec, r2_cond3[v], t3_vec)))  # low
+        # high = t5 + d56 * sel0
+        for v in range(NVECS):
+            r2_all.append(("valu", ("multiply_add", r2_cond6[v], five_vec, r2_cond3[v], t5_vec)))  # high
+        # diff_high = high - low
+        for v in range(NVECS):
+            r2_all.append(("valu", ("-", r2_tmp[v], r2_cond6[v], r2_cond5[v])))  # diff_high
+        # node = low + diff_high * sel1
+        for v in range(NVECS):
+            r2_all.append(("valu", ("multiply_add", node_vecs[v], r2_tmp[v], r2_cond4[v], r2_cond5[v])))
+        # Compute R2 (no wrap needed - max idx after R2 is 14 < n_nodes)
+        r2_all.extend(gen_xor(val_vecs[:4], node_vecs[:4]))
+        r2_all.extend(gen_xor(val_vecs[4:], node_vecs[4:]))
+        r2_all.extend(gen_hash(val_vecs[:4], temps_A))
+        r2_all.extend(gen_hash(val_vecs[4:], temps_B))
+        # Do idx_update for group A first (needed for R3 prime gather)
+        r2_all.extend(gen_index_update(idx_vecs[:4], val_vecs[:4], temps_A, n_nodes_vec, with_wrap=False))
+        self.instrs.extend(self._pack_slots(r2_all))
+
+        # --- Pipeline R3..15 ---
+        # Wrap only needed for rounds >= 9 (max idx after round 8 is 1022 < 1023)
+
+        # Prime R3 + R2 idx_update for group B (interleaved)
+        # R3 prime gathers group A, so we can overlap with R2's idx_update for group B
+        prime_ops = []
+        # Gather addr calc for group A
+        for i in range(4):
+            prime_ops.append(("valu", ("+", addr_vecs[i], idx_vecs[i], forest_base_vec)))
+        # Gather loads for A and idx_update for B - interleave 2:6
+        gather_loads = []
+        for i in range(4):
+            for vi in range(VLEN):
+                gather_loads.append(("load", ("load_offset", node_vecs[i], addr_vecs[i], vi)))
+        idx_update_B = gen_index_update(idx_vecs[4:], val_vecs[4:], temps_B, n_nodes_vec, with_wrap=False)
+
+        load_idx = 0
+        hash_idx = 0
+        while load_idx < len(gather_loads) or hash_idx < len(idx_update_B):
+            for _ in range(2):
+                if load_idx < len(gather_loads):
+                    prime_ops.append(gather_loads[load_idx])
+                    load_idx += 1
+            for _ in range(6):
+                if hash_idx < len(idx_update_B):
+                    prime_ops.append(idx_update_B[hash_idx])
+                    hash_idx += 1
+        self.instrs.extend(self._pack_slots(prime_ops))
+
+        # Loop R3..15
+        for r in range(3, rounds):
+            # Wrap needed when completing round r where r >= 9
+            needs_wrap = (r >= 9)
+            # Step 1: Hash A(r) | Gather B(r)
+            emit_pipelined_step(gatherA=False, hashA=True, gatherB=True, hashB=False, with_wrap=needs_wrap)
+
+            # Step 2: Gather A(r+1) | Hash B(r)
+            next_gather = (r + 1 < rounds)
+            if next_gather:
+                # Normal case: gatherA(r+1) | hashB(r)
+                emit_pipelined_step(gatherA=True, hashA=False, gatherB=False, hashB=True, with_wrap=needs_wrap)
+            else:
+                # Last round (r=15): no gatherA, so interleave hashB with stores for group A
+                last_ops = []
+                hash_ops_B = gen_xor(val_vecs[4:], node_vecs[4:])
+                hash_ops_B.extend(gen_hash(val_vecs[4:], temps_B))
+                hash_ops_B.extend(gen_index_update(idx_vecs[4:], val_vecs[4:], temps_B, n_nodes_vec, with_wrap=needs_wrap))
+
+                # Stores for group A (8 vstores = 4 cycles at 2/cycle)
+                store_ops_A = []
+                for i in range(4):
+                    store_ops_A.append(("store", ("vstore", ptrs_i[i], idx_vecs[i])))
+                    store_ops_A.append(("store", ("vstore", ptrs_v[i], val_vecs[i])))
+
+                # Interleave: 2 stores per 6 valu
+                store_idx = 0
+                hash_idx = 0
+                while store_idx < len(store_ops_A) or hash_idx < len(hash_ops_B):
+                    for _ in range(2):
+                        if store_idx < len(store_ops_A):
+                            last_ops.append(store_ops_A[store_idx])
+                            store_idx += 1
+                    for _ in range(6):
+                        if hash_idx < len(hash_ops_B):
+                            last_ops.append(hash_ops_B[hash_idx])
+                            hash_idx += 1
+                self.instrs.extend(self._pack_slots(last_ops))
+
+        # Store group B + Jump control (group A already stored above)
+        jump_cond = self.alloc_scratch("jump_cond")
+        store_ops = []
+        for i in range(4, NVECS):
+             store_ops.append(("store", ("vstore", ptrs_i[i], idx_vecs[i])))
+             store_ops.append(("store", ("vstore", ptrs_v[i], val_vecs[i])))
+        # Overlap jump control with stores
+        store_ops.append(("alu", ("+", batch_counter, batch_counter, vlen8_const)))
+        store_ops.append(("alu", ("<", jump_cond, batch_counter, batch_end)))
+        self.instrs.extend(self._pack_slots(store_ops))
+        
+        jump_src = len(self.instrs) + 1
+        offset = batch_loop_start - jump_src
+        self.add("flow", ("cond_jump_rel", jump_cond, offset))
         self.add("flow", ("pause",))
-        # Any debug engine instruction is ignored by the submission simulator
-        self.add("debug", ("comment", "Starting loop"))
-
-        body = []  # array of slots
-
-        # Scalar scratch registers
-        tmp_idx = self.alloc_scratch("tmp_idx")
-        tmp_val = self.alloc_scratch("tmp_val")
-        tmp_node_val = self.alloc_scratch("tmp_node_val")
-        tmp_addr = self.alloc_scratch("tmp_addr")
-
-        for round in range(rounds):
-            for i in range(batch_size):
-                i_const = self.scratch_const(i)
-                # idx = mem[inp_indices_p + i]
-                body.append(("alu", ("+", tmp_addr, self.scratch["inp_indices_p"], i_const)))
-                body.append(("load", ("load", tmp_idx, tmp_addr)))
-                body.append(("debug", ("compare", tmp_idx, (round, i, "idx"))))
-                # val = mem[inp_values_p + i]
-                body.append(("alu", ("+", tmp_addr, self.scratch["inp_values_p"], i_const)))
-                body.append(("load", ("load", tmp_val, tmp_addr)))
-                body.append(("debug", ("compare", tmp_val, (round, i, "val"))))
-                # node_val = mem[forest_values_p + idx]
-                body.append(("alu", ("+", tmp_addr, self.scratch["forest_values_p"], tmp_idx)))
-                body.append(("load", ("load", tmp_node_val, tmp_addr)))
-                body.append(("debug", ("compare", tmp_node_val, (round, i, "node_val"))))
-                # val = myhash(val ^ node_val)
-                body.append(("alu", ("^", tmp_val, tmp_val, tmp_node_val)))
-                body.extend(self.build_hash(tmp_val, tmp1, tmp2, round, i))
-                body.append(("debug", ("compare", tmp_val, (round, i, "hashed_val"))))
-                # idx = 2*idx + (1 if val % 2 == 0 else 2)
-                body.append(("alu", ("%", tmp1, tmp_val, two_const)))
-                body.append(("alu", ("==", tmp1, tmp1, zero_const)))
-                body.append(("flow", ("select", tmp3, tmp1, one_const, two_const)))
-                body.append(("alu", ("*", tmp_idx, tmp_idx, two_const)))
-                body.append(("alu", ("+", tmp_idx, tmp_idx, tmp3)))
-                body.append(("debug", ("compare", tmp_idx, (round, i, "next_idx"))))
-                # idx = 0 if idx >= n_nodes else idx
-                body.append(("alu", ("<", tmp1, tmp_idx, self.scratch["n_nodes"])))
-                body.append(("flow", ("select", tmp_idx, tmp1, tmp_idx, zero_const)))
-                body.append(("debug", ("compare", tmp_idx, (round, i, "wrapped_idx"))))
-                # mem[inp_indices_p + i] = idx
-                body.append(("alu", ("+", tmp_addr, self.scratch["inp_indices_p"], i_const)))
-                body.append(("store", ("store", tmp_addr, tmp_idx)))
-                # mem[inp_values_p + i] = val
-                body.append(("alu", ("+", tmp_addr, self.scratch["inp_values_p"], i_const)))
-                body.append(("store", ("store", tmp_addr, tmp_val)))
-
-        body_instrs = self.build(body)
-        self.instrs.extend(body_instrs)
-        # Required to match with the yield in reference_kernel2
-        self.instrs.append({"flow": [("pause",)]})
 
 BASELINE = 147734
 
@@ -185,88 +698,42 @@ def do_kernel_test(
     forest = Tree.generate(forest_height)
     inp = Input.generate(forest, batch_size, rounds)
     mem = build_mem_image(forest, inp)
+    mem_for_machine = list(mem)
+    
+    ref_gen = reference_kernel2(mem, {})
+    next(ref_gen) 
+    ref_final = next(ref_gen)
 
     kb = KernelBuilder()
     kb.build_kernel(forest.height, len(forest.values), len(inp.indices), rounds)
-    # print(kb.instrs)
 
-    value_trace = {}
     machine = Machine(
-        mem,
+        mem_for_machine,
         kb.instrs,
         kb.debug_info(),
         n_cores=N_CORES,
-        value_trace=value_trace,
         trace=trace,
     )
     machine.prints = prints
-    for i, ref_mem in enumerate(reference_kernel2(mem, value_trace)):
-        machine.run()
-        inp_values_p = ref_mem[6]
-        if prints:
-            print(machine.mem[inp_values_p : inp_values_p + len(inp.values)])
-            print(ref_mem[inp_values_p : inp_values_p + len(inp.values)])
-        assert (
-            machine.mem[inp_values_p : inp_values_p + len(inp.values)]
-            == ref_mem[inp_values_p : inp_values_p + len(inp.values)]
-        ), f"Incorrect result on round {i}"
-        inp_indices_p = ref_mem[5]
-        if prints:
-            print(machine.mem[inp_indices_p : inp_indices_p + len(inp.indices)])
-            print(ref_mem[inp_indices_p : inp_indices_p + len(inp.indices)])
-        # Updating these in memory isn't required, but you can enable this check for debugging
-        # assert machine.mem[inp_indices_p:inp_indices_p+len(inp.indices)] == ref_mem[inp_indices_p:inp_indices_p+len(inp.indices)]
+    machine.run()
+    
+    for _ in ref_gen: pass
+    
+    inp_values_p = ref_final[6]
+    inp_indices_p = ref_final[5]
+    
+    m_vals = machine.mem[inp_values_p : inp_values_p + len(inp.values)]
+    r_vals = ref_final[inp_values_p : inp_values_p + len(inp.values)]
+    
+    assert m_vals == r_vals, "Incorrect result values"
+    
+    ind_m = machine.mem[inp_indices_p : inp_indices_p + len(inp.indices)]
+    ind_r = ref_final[inp_indices_p : inp_indices_p + len(inp.indices)]
+    assert ind_m == ind_r, "Incorrect result indices"
 
     print("CYCLES: ", machine.cycle)
     print("Speedup over baseline: ", BASELINE / machine.cycle)
     return machine.cycle
 
-
-class Tests(unittest.TestCase):
-    def test_ref_kernels(self):
-        """
-        Test the reference kernels against each other
-        """
-        random.seed(123)
-        for i in range(10):
-            f = Tree.generate(4)
-            inp = Input.generate(f, 10, 6)
-            mem = build_mem_image(f, inp)
-            reference_kernel(f, inp)
-            for _ in reference_kernel2(mem, {}):
-                pass
-            assert inp.indices == mem[mem[5] : mem[5] + len(inp.indices)]
-            assert inp.values == mem[mem[6] : mem[6] + len(inp.values)]
-
-    def test_kernel_trace(self):
-        # Full-scale example for performance testing
-        do_kernel_test(10, 16, 256, trace=True, prints=False)
-
-    # Passing this test is not required for submission, see submission_tests.py for the actual correctness test
-    # You can uncomment this if you think it might help you debug
-    # def test_kernel_correctness(self):
-    #     for batch in range(1, 3):
-    #         for forest_height in range(3):
-    #             do_kernel_test(
-    #                 forest_height + 2, forest_height + 4, batch * 16 * VLEN * N_CORES
-    #             )
-
-    def test_kernel_cycles(self):
-        do_kernel_test(10, 16, 256)
-
-
-# To run all the tests:
-#    python perf_takehome.py
-# To run a specific test:
-#    python perf_takehome.py Tests.test_kernel_cycles
-# To view a hot-reloading trace of all the instructions:  **Recommended debug loop**
-# NOTE: The trace hot-reloading only works in Chrome. In the worst case if things aren't working, drag trace.json onto https://ui.perfetto.dev/
-#    python perf_takehome.py Tests.test_kernel_trace
-# Then run `python watch_trace.py` in another tab, it'll open a browser tab, then click "Open Perfetto"
-# You can then keep that open and re-run the test to see a new trace.
-
-# To run the proper checks to see which thresholds you pass:
-#    python tests/submission_tests.py
-
 if __name__ == "__main__":
-    unittest.main()
+    do_kernel_test(10, 16, 256)
